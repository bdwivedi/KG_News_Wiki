{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cced82-df67-4db3-be0c-95c03a359e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/explosion/spacy-models/releases//download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed8c785-59fe-4a2c-abbc-a15fd5fa97cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1e4b40b4688>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #en_core_web_lg\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc8e402-e78e-4f7e-8934-3f6f39c6cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textacy.subject_verb_object_extract import findSVOs\n",
    "from subject_verb_object_extract import findSVOs\n",
    "from src_rel_tgt_extract import findSVAOs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "punct='!\"\"#$%&\\'()*+-/:;<=>?@[\\\\]^_”’“`{|}~``''—'  \n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "l_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "import py_stringmatching as sm\n",
    " \n",
    "SW=sm.SmithWaterman()\n",
    "GJ=sm.GeneralizedJaccard()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()\n",
    "ocf=sm.OverlapCoefficient()\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_ratio import PartialRatio\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_token_sort import PartialTokenSort\n",
    "pr= PartialRatio()\n",
    "pts= PartialTokenSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6513edb7-b32b-4c81-a3a1-6a934223e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8af72d-c491-4904-ad7c-590f5d6913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_ent_list = [ 'CARDINAL', 'DATE','WORK_OF_ART','MONEY','ORDINAL','FAC','TIME','LOC','QUANTITY','LAW','PERCENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc4981b-c161-4089-8fc5-28f7979d1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlalchemy as sal\n",
    "# from sqlalchemy import text\n",
    "# engine = sal.create_engine('postgresql+psycopg2://ag_class:WUcgdfQ1@awesome-hw.sdsc.edu/postgres')\n",
    "# conn = engine.connect()\n",
    "# sqlquery2 = text('''SELECT *\n",
    "#   FROM\n",
    "#       usnewspaper \n",
    "#   WHERE news LIKE '%chip shortage%' and collectiondate > '2022-01-01';''')\n",
    "# df = pd.read_sql(sqlquery2, engine)\n",
    "# df.to_csv('semi_cs_news_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ddb72-bc3e-4105-9bc0-70f29d26a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af697674-a206-4198-96ce-074be8e5b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_data=pd.read_csv(\"semi_cs_news_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fd3b7a-5c28-4243-ad53-d31b752236b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10635"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe32ac2-460d-4412-a0e0-418249bb3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data = df_news_data.iloc[:50,:]\n",
    "sample_data = df_news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75929dcb-8c5e-403e-8fef-1cee3be42ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = sample_data.loc[30]['news']\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c675af39-ba2e-46ac-b46f-4c40e62c943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_title(data):\n",
    "    mod_list = []\n",
    "    for x in data.split('\\n\\n'):\n",
    "        word_count = len(x.split())\n",
    "        if word_count > 15:\n",
    "            # print(x, '->', word_count )\n",
    "            mod_list.append(x)\n",
    "    final_text = ''.join(mod_list)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48391c5-30fa-4f03-8ec9-9ffdb60d7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below code calculates the subject verb and object \n",
    "def create_svo_ent_tkn(sample_data):\n",
    "    doc_sub_v_sub=list() \n",
    "    root_verbs=list()\n",
    "    token_entity=list()\n",
    "\n",
    "    for j in range(( sample_data.shape[0] )):\n",
    "        data = sample_data.loc[j]['news']\n",
    "        sample_data_new = rem_title(data)\n",
    "        int_text=[x.translate(str.maketrans('','',punct)) for x in  sample_data_new.split()]\n",
    "\n",
    "        doc=nlp(' '.join([str(elem) for elem in int_text]))\n",
    "        #t_txt=textacy.preprocess_text(sample_data.loc[ j]['news'],lowercase=True,no_punct=True )\n",
    "        #doc=nlp(sample_data.loc[651+j]['news']) #performing the nlp with any preprocessing\n",
    "        for tok in doc:\n",
    "            if tok.dep_=='ROOT' and tok.pos_=='VERB':\n",
    "                root_verbs.append(str(tok))\n",
    "\n",
    "        # To solve coref issue\n",
    "        doc_coref_resolved=nlp(doc._.coref_resolved)\n",
    "\n",
    "        #Identifying entity types. Need some new code to add new entities\n",
    "        for tok in doc_coref_resolved.ents:\n",
    "            if tok.label_ not in reject_ent_list:\n",
    "                token_entity.append([str(tok),tok.label_])\n",
    "        \n",
    "        SVO=[]\n",
    "        #SVO using a custom code to handle active and passive voice\n",
    "        for sent in doc_coref_resolved.sents:\n",
    "            text = findSVOs(sent)\n",
    "            SVO.append(text)\n",
    "        doc_sub_v_sub.append([item for sublist in SVO for item in sublist if item != []])\n",
    "\n",
    "    ## The below snippets appends needed columns from the news data with the subject object and verb tripples    \n",
    "    l_sub_v_sub=list()\n",
    "    for i in range(len(doc_sub_v_sub)):\n",
    "        new=list(sample_data.loc[i].values)[:4]\n",
    "\n",
    "        for j in range(len(doc_sub_v_sub[i])):\n",
    "            #df_needed_fields.append([new+[t for t in  doc_sub_v_sub[i][j]]])\n",
    "            l_sub_v_sub.append(new+[str(sub_obj) for sub_obj in  doc_sub_v_sub[i][j]]) \n",
    "     \n",
    "    return l_sub_v_sub, token_entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b56c0b-e472-4126-81db-f6c1fbb53f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 57min 54s, sys: 11min 31s, total: 2h 9min 26s\n",
      "Wall time: 2h 9min 55s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# sub_vb_obj, token_entity = create_svo_ent_tkn(sample_data)\n",
    "# #create the SVO and the Entity Token dataframe\n",
    "# df_doc_sub_v_sub=pd.DataFrame(sub_vb_obj,columns=['id','src','published_date','title','subject','verb','object'])\n",
    "# df_token_entity=pd.DataFrame(token_entity,columns=['Token','Entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ffb772-065a-4447-b7ba-a45539fcd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_doc_sub_v_sub), len(df_token_entity)\n",
    "# df_token_entity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6163eeb-15b5-4fef-b30a-498e26a60834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doc_sub_v_sub.to_csv('Data/sub_vb_ob_lib2_news_5k.csv', index=False)\n",
    "# df_token_entity.to_csv('Data/ent_tok_lib2_news_5k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1016490-8827-47c3-9e94-c92fb1543a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_token_entity.Entity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d533133e-8cd9-4bd0-991b-c0222ad096f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_sub_v_sub = pd.read_csv('Data/sub_vb_ob_lib2_news.csv')\n",
    "df_token_entity = pd.read_csv('Data/ent_tok_lib2_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cacc60b-073f-4b9f-b1ec-eac5548391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_before_stop = pd.read_csv('Data/sub_vb_ob_lib2_news_5k.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ce8657-e77a-4d86-a470-33dccd5facbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_before_stop[['subject','verb','object']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e647e2e6-aebc-4e1a-9956-10b5e8f1b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 360868\n"
     ]
    }
   ],
   "source": [
    "# The below dataframe lowers the tokens in both the dataframe\n",
    "df_token_entity['Token']=df_token_entity['Token'].str.lower()\n",
    "df_token_entity=df_token_entity.drop_duplicates()\n",
    "df_token_entity=df_token_entity.drop_duplicates(subset='Token', keep=\"first\")\n",
    "df_doc_sub_v_sub['subject']=df_doc_sub_v_sub['subject'].str.lower()\n",
    "df_doc_sub_v_sub['object']=df_doc_sub_v_sub['object'].str.lower()\n",
    "print('1.',len(df_doc_sub_v_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4323c0-91d4-4104-8419-d96255647a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 360868\n",
      "3. 360868\n",
      "4. 7141\n"
     ]
    }
   ],
   "source": [
    "#Creating the sub verb and object dataframe and joining the entity to find the subject and object entities\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub.merge(df_token_entity,how='left', left_on='subject',right_on='Token')\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub.merge(df_token_entity,how='left', left_on='object',right_on='Token')\n",
    "print('2.',len(df_doc_sub_v_sub))\n",
    "\n",
    "#renaming and dropping unnecessary columns\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub.rename(columns={\"Entity_x\":\"Subject_Entity\",\"Entity_y\":\"Object_Entity\"})\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub.drop(columns=['Token_x','Token_y'])\n",
    "print('3.',len(df_doc_sub_v_sub))\n",
    "\n",
    "# Removing rows that does not have an entity defined and where subject and object are the same\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub[df_doc_sub_v_sub['Subject_Entity'].notnull() & df_doc_sub_v_sub['Object_Entity'].notnull()]\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub[df_doc_sub_v_sub['subject'].notnull() & df_doc_sub_v_sub['object'].notnull()]\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub[df_doc_sub_v_sub['subject']!=df_doc_sub_v_sub['object']]\n",
    "print('4.',len(df_doc_sub_v_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bb688-752f-429b-a654-8eebe2f3f595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb07d109-7194-4812-8e40-696e9281c8dd",
   "metadata": {},
   "source": [
    "#### stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75597433-19a6-43a6-8a91-ecc4ff587812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spacy_stop=list(nlp.Defaults.stop_words)\n",
    "complete_stop_word=list(set(l_stopwords+spacy_stop))\n",
    "\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub[~df_doc_sub_v_sub['verb'].isin(complete_stop_word)]\n",
    "\n",
    "data_verbs=df_doc_sub_v_sub['verb'].to_list()\n",
    "lemm_verbs=[]\n",
    "j=0\n",
    "for i in (data_verbs):\n",
    "    doc_verb=nlp(i)\n",
    "    verbs=[]\n",
    "    for tok in doc_verb:\n",
    "        verbs.append(tok.lemma_)\n",
    "    verbs=' '.join(verbs)\n",
    "    lemm_verbs.append(verbs)\n",
    "    j=j+1\n",
    "    if(j%10000==0):\n",
    "        print(j)\n",
    "df_doc_sub_v_sub['lemm_verbs']=lemm_verbs\n",
    "df_doc_sub_v_sub=df_doc_sub_v_sub[~df_doc_sub_v_sub['lemm_verbs'].isin(complete_stop_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391ec60a-8e99-4005-8dd1-2a385ef3dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5910\n"
     ]
    }
   ],
   "source": [
    "print(len(df_doc_sub_v_sub)) #.Subject_Entity.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d0a10-71d0-40aa-9321-346dfe4d638c",
   "metadata": {},
   "source": [
    "#### Find Word Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac795f7-ea10-4dc6-8e51-08648f880d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5. 5910\n"
     ]
    }
   ],
   "source": [
    "# import spacy as spacy_nlp\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "# spacy_nlp = spacy.load('en')\n",
    "nlp_lemma_word_ann = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# nlp_lemma_word_ann.add_pipe(WordnetAnnotator(spacy_nlp.lang), after='tagger')\n",
    "nlp_lemma_word_ann.add_pipe(WordnetAnnotator(nlp_lemma_word_ann, name=\"spacy_wordnet\"), after='tagger')\n",
    "# nlp_lemma_word_ann.add_pipe(WordnetAnnotator(nlp_lemma_word_ann.lang), after='tagger')\n",
    "\n",
    "\n",
    "nlp_lemma_NO_word_annot = spacy.load('en_core_web_sm', disable=['parser', 'ner','WordnetAnnotator'])\n",
    "\n",
    "\n",
    "lemm_verbs_no_stopwords=df_doc_sub_v_sub['lemm_verbs'].to_list()\n",
    "#lemm_verbs_no_stopwords=[i.replace('! ', '') for i in lemm_verbs_no_stopwords]\n",
    "sorted_lemma_count=sorted(Counter(lemm_verbs_no_stopwords).items(), key=operator.itemgetter(1),reverse=True) \n",
    " \n",
    "#Get just the verbs from the previous list and ignore the verb count\n",
    "lemm_verbs_synonym=[a_tuple[0] for a_tuple in sorted_lemma_count]\n",
    "all_synonyms = []\n",
    "prev_all_synonyms=[]\n",
    "synn_list=[]\n",
    "#Find the synonyms of the top verbs and replace that verb if any synonym is found\n",
    "for i in range(len(sorted_lemma_count)):\n",
    "     \n",
    "    #all_synonyms=[item for sublist in all_synonyms for item in sublist]\n",
    "     \n",
    "    if sorted_lemma_count[i][0] not in prev_all_synonyms:#Check is needed so that the same word does not get updated\n",
    "        #print(sorted_lemma_count[i][0])\n",
    "        if (len(sorted_lemma_count[i][0].split())==1):\n",
    "            for word in  nlp_lemma_word_ann(sorted_lemma_count[i][0]):\n",
    "                synonyms = []\n",
    "                for sysnet in word._.wordnet.synsets():\n",
    "\n",
    "                    for l in sysnet.lemmas():\n",
    "                        synonyms.append(l.name())\n",
    "                        all_synonyms.append(l.name())\n",
    "            synonyms=list(set(synonyms))\n",
    "            #print(synonyms)\n",
    "            all_synonyms=list(set(all_synonyms))\n",
    "            synn_list.append((sorted_lemma_count[i][0],synonyms))\n",
    "\n",
    "            for n,j in enumerate(lemm_verbs_no_stopwords):\n",
    "                #print(j,prev_all_synonyms)\n",
    "                if j in synonyms and j not in prev_all_synonyms : #and lemm_verbs_no_stopwords[n]!=sorted_lemma_count[i][0]\n",
    "                    #print(sorted_lemma_count[i][0],j)\n",
    "                    if (len(j.split())==1):\n",
    "                    #print(j,sorted_lemma_count[i][0])\n",
    "\n",
    "                    #idx=lemm_verbs_no_stopwords.index(j)\n",
    "                    #print(j ,sorted_lemma_count[i][0])\n",
    "                        #print(j,'before',lemm_verbs_no_stopwords[n],'after',sorted_lemma_count[i][0])\n",
    "                        lemm_verbs_no_stopwords[n]=sorted_lemma_count[i][0]\n",
    "                        #print('after',lemm_verbs_no_stopwords[n])\n",
    "              \n",
    "            prev_all_synonyms=all_synonyms\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "df_doc_sub_v_sub['synn_verbs']=lemm_verbs_no_stopwords\n",
    "print('5.',len(df_doc_sub_v_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1aaabf-91b4-4806-ac9d-0f550914a9ee",
   "metadata": {},
   "source": [
    "### Similarity measure between subject & object token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c8806f4-e21d-49a1-a331-ff743bd08044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating subject and object list\n",
    "subject_list=df_doc_sub_v_sub['subject'].to_list()\n",
    "subject_list=[item.lower() for item in subject_list]\n",
    "\n",
    "\n",
    "object_list=df_doc_sub_v_sub['object'].to_list()\n",
    "object_list=[item.lower() for item in object_list]\n",
    " \n",
    "full_s_o_list=subject_list+object_list\n",
    "\n",
    "\n",
    "import py_stringmatching as sm\n",
    " \n",
    "SW=sm.SmithWaterman()\n",
    "GJ=sm.GeneralizedJaccard()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()\n",
    "ocf=sm.OverlapCoefficient()\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_ratio import PartialRatio\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_token_sort import PartialTokenSort\n",
    "pr= PartialRatio()\n",
    "pts= PartialTokenSort()\n",
    "\n",
    "\n",
    "full_s_o_list_count=sorted(Counter(full_s_o_list).items(), key=operator.itemgetter(1),reverse=True) \n",
    "\n",
    "#Get just the subject and object from the previous list and ignore the  count\n",
    "full_s_o_list_final=[a_tuple[0] for a_tuple in full_s_o_list_count] \n",
    "\n",
    "\n",
    "ws_tok_set= sm.WhitespaceTokenizer(return_set=True)\n",
    "calc_score_temp=[]\n",
    "all_sym=[]\n",
    "sim_lis=[]\n",
    "prev_all_sym=[]\n",
    "  \n",
    "for i in range(len(full_s_o_list_final)):\n",
    "     \n",
    "    if full_s_o_list_final[i]  not in all_sym: #Check is needed so that the same word does not get updated\n",
    "        similarity_list = []\n",
    "        calc_score_temp=[]\n",
    "        for j in range(len(full_s_o_list_final)):\n",
    "             \n",
    "            # score=(2*(SW.get_raw_score( full_s_o_list_final[i],full_s_o_list_final[j]))/(len(full_s_o_list_final[i])+len(full_s_o_list_final[j])))\n",
    "            score = jac.get_raw_score(ws_tok_set.tokenize(full_s_o_list_final[i]), ws_tok_set.tokenize(full_s_o_list_final[j]))\n",
    "            calc_score_temp.append((full_s_o_list_final[i],full_s_o_list_final[j],score))\n",
    "            \n",
    "            if score > .5:\n",
    "                all_sym.append(full_s_o_list_final[j])\n",
    "                similarity_list.append(full_s_o_list_final[j])\n",
    "        sim_lis.append((full_s_o_list_final[i],similarity_list))\n",
    "        for n,k in enumerate(subject_list):\n",
    "            if k in similarity_list and k not in prev_all_sym :\n",
    "                 subject_list[n]=full_s_o_list_final[i]\n",
    "        for n,k in enumerate(object_list):\n",
    "            if k in similarity_list and k not in prev_all_sym :\n",
    "                object_list[n]=full_s_o_list_final[i]\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6077c3e8-0de2-47da-9b4e-72d675de43f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', ['we']),\n",
       " ('tesla', ['tesla']),\n",
       " ('companies', ['companies']),\n",
       " ('sales', ['sales']),\n",
       " ('apple', ['apple']),\n",
       " ('demand', ['demand']),\n",
       " ('intel', ['intel']),\n",
       " ('vehicles', ['vehicles']),\n",
       " ('consumers', ['consumers']),\n",
       " ('us', ['us']),\n",
       " ('earnings', ['earnings']),\n",
       " ('ford', ['ford']),\n",
       " ('inflation', ['inflation']),\n",
       " ('supply', ['supply'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_lis[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a005e60-bcbc-4924-b6a1-39ce414e71bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. 5908\n"
     ]
    }
   ],
   "source": [
    "df_doc_sub_v_sub['subject_sim']=subject_list\n",
    "df_doc_sub_v_sub['object_sim']=object_list\n",
    "\n",
    "df_doc_sub_v_sub_final=df_doc_sub_v_sub[df_doc_sub_v_sub['subject_sim']!=df_doc_sub_v_sub['object_sim']] \n",
    "print('6.',len(df_doc_sub_v_sub_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8918df1-2cfa-418b-aed2-ddfe1e37c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. 5908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdivedi\\Anaconda3\\envs\\nlp_proj\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_doc_sub_v_sub_final['verb_source_type']='News'\n",
    "\n",
    "df_doc_sub_v_sub_graph_out=df_doc_sub_v_sub_final[['subject_sim','Subject_Entity','object_sim','Object_Entity','synn_verbs','published_date','verb_source_type','src']]\n",
    "df_doc_sub_v_sub_graph_out=df_doc_sub_v_sub_graph_out.rename(columns={'subject_sim':'subject','Subject_Entity':'subject_entity','object_sim':'object','Object_Entity':'object_entity','synn_verbs':'synn_verbs','published_date':'created_at','src':'verb_source_name'})\n",
    "print('7.',len(df_doc_sub_v_sub_graph_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3fde9c8-9899-40b8-a628-7be80af8fcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. 5109\n"
     ]
    }
   ],
   "source": [
    "df_doc_sub_v_sub_graph_out = df_doc_sub_v_sub_graph_out[(df_doc_sub_v_sub_graph_out['object'].str.len()>2) & (df_doc_sub_v_sub_graph_out['subject'].str.len()>2)] \n",
    "df_doc_sub_v_sub_graph_out = df_doc_sub_v_sub_graph_out[(df_doc_sub_v_sub_graph_out['object'].str.len()>2) & (df_doc_sub_v_sub_graph_out['subject'].str.len()>2)] \n",
    "print('8.',len(df_doc_sub_v_sub_graph_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28b1574e-7c92-46f3-90b1-bad7a1364701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doc_sub_v_sub_graph_out[\"subject_entity\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc3bc1b-7c51-413b-9dc0-66213a726bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. 5109\n"
     ]
    }
   ],
   "source": [
    "df_graph_news = df_doc_sub_v_sub_graph_out[['subject','object','synn_verbs','subject_entity','object_entity','verb_source_type']]\n",
    "df_graph_news.columns = [':START_ID',':END_ID',':TYPE','subject_entity','object_entity','source_type']\n",
    "print('9.',len(df_graph_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40526096-7026-4229-ac9b-7700d23d5314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>:START_ID</th>\n",
       "      <th>:END_ID</th>\n",
       "      <th>:TYPE</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>source_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>goldman sachs gs.n</td>\n",
       "      <td>marvell</td>\n",
       "      <td>upgrade</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>apple</td>\n",
       "      <td>vendors</td>\n",
       "      <td>instruct</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>abada</td>\n",
       "      <td>bitfly</td>\n",
       "      <td>recommend</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>goldman sachs gs.n</td>\n",
       "      <td>marvell</td>\n",
       "      <td>upgrade</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>south africa</td>\n",
       "      <td>sales</td>\n",
       "      <td>run</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              :START_ID  :END_ID      :TYPE subject_entity object_entity  \\\n",
       "11   goldman sachs gs.n  marvell    upgrade            ORG           ORG   \n",
       "93                apple  vendors   instruct            ORG       PRODUCT   \n",
       "178               abada   bitfly  recommend         PERSON        PERSON   \n",
       "517  goldman sachs gs.n  marvell    upgrade            ORG           ORG   \n",
       "680        south africa    sales        run            GPE       PRODUCT   \n",
       "\n",
       "    source_type  \n",
       "11         News  \n",
       "93         News  \n",
       "178        News  \n",
       "517        News  \n",
       "680        News  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_graph_news=df_graph_news[df_graph_news[':END_ID']!=df_graph_news[':TYPE']] \n",
    "df_graph_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37380be9-5ecd-41cb-8873-935991fda619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5107"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_graph_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b0b00cd-833b-455b-88a2-5586a48e3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_graph_news = df_graph_news[df_graph_news['object_entity'] != 'NORP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e27b4c-022a-4968-a308-1f4e34840d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>:START_ID</th>\n",
       "      <th>:END_ID</th>\n",
       "      <th>:TYPE</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>source_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357496</th>\n",
       "      <td>money</td>\n",
       "      <td>hero enterprise</td>\n",
       "      <td>pour</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357504</th>\n",
       "      <td>aggarwal</td>\n",
       "      <td>ola electric</td>\n",
       "      <td>spearhead</td>\n",
       "      <td>GPE</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358505</th>\n",
       "      <td>retailers</td>\n",
       "      <td>consumers</td>\n",
       "      <td>urge</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>GPE</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358518</th>\n",
       "      <td>retailers</td>\n",
       "      <td>shoppers</td>\n",
       "      <td>encourage</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358710</th>\n",
       "      <td>freeman</td>\n",
       "      <td>the biden administration</td>\n",
       "      <td>convince</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358759</th>\n",
       "      <td>fidelity</td>\n",
       "      <td>money</td>\n",
       "      <td>raise</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359034</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>capacity</td>\n",
       "      <td>increase</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359047</th>\n",
       "      <td>workers</td>\n",
       "      <td>ford</td>\n",
       "      <td>bring</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359435</th>\n",
       "      <td>users</td>\n",
       "      <td>nio evs</td>\n",
       "      <td>buy</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359537</th>\n",
       "      <td>companies</td>\n",
       "      <td>investment</td>\n",
       "      <td>cut</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        :START_ID                   :END_ID      :TYPE subject_entity  \\\n",
       "357496      money           hero enterprise       pour            ORG   \n",
       "357504   aggarwal              ola electric  spearhead            GPE   \n",
       "358505  retailers                 consumers       urge         PERSON   \n",
       "358518  retailers                  shoppers  encourage         PERSON   \n",
       "358710    freeman  the biden administration   convince            ORG   \n",
       "358759   fidelity                     money      raise            ORG   \n",
       "359034       cnbc                  capacity   increase            ORG   \n",
       "359047    workers                      ford      bring            ORG   \n",
       "359435      users                   nio evs        buy            ORG   \n",
       "359537  companies                investment        cut            ORG   \n",
       "\n",
       "       object_entity source_type  \n",
       "357496           ORG        News  \n",
       "357504           ORG        News  \n",
       "358505           GPE        News  \n",
       "358518        PERSON        News  \n",
       "358710           ORG        News  \n",
       "358759           ORG        News  \n",
       "359034           ORG        News  \n",
       "359047           ORG        News  \n",
       "359435       PRODUCT        News  \n",
       "359537           ORG        News  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_graph_news = df_graph_news.drop_duplicates()\n",
    "df_graph_news.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a57c11d2-a276-4e5e-9307-1860ac15efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. 1386\n"
     ]
    }
   ],
   "source": [
    "print('10.',len(df_graph_news))\n",
    "df_graph_news.to_csv('Data/node_news_1208.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d124877f-c610-4f03-85e2-4ce6b3066c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make SRC as unique for various nodes like ford co to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990149a-3e3d-4c03-9663-cb585248c85b",
   "metadata": {},
   "source": [
    "##### Generate Node on News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7da0a251-cf84-4a53-8819-846f03bd9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn1 = df_doc_sub_v_sub_graph_out[['subject','subject_entity']]\n",
    "# df_tkn1.columns = ['name',':LABEL']\n",
    "# df_tkn2 = df_doc_sub_v_sub_graph_out[['object','object_entity']]\n",
    "# df_tkn2.columns = ['name',':LABEL']\n",
    "# df_tkn3 = df_tkn_ent_wiki[['subject','subject_entity']]\n",
    "# df_tkn3.columns = ['name',':LABEL']\n",
    "# df_tkn4 = df_tkn_ent_wiki[['object','object_entity']]\n",
    "# df_tkn4.columns = ['name',':LABEL']\n",
    "\n",
    "# df_tkn_merged = pd.concat([df_tkn1,df_tkn2,df_tkn3,df_tkn4], ignore_index=True)\n",
    "# df_tkn_merged = df_tkn_merged.drop_duplicates(subset=['name'])\n",
    "# df_tkn_merged.reset_index(inplace = True)\n",
    "# df_tkn_merged.reset_index(inplace = True)\n",
    "# df_tkn_merged = df_tkn_merged.rename(columns={'level_0':'Id:ID'})\n",
    "# df_tkn_merged['Id:ID'] = df_tkn_merged['name'].apply(lambda x : x.upper())\n",
    "# df_tkn_merged = df_tkn_merged.drop('index',axis=1)\n",
    "# df_tkn_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "nlp_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
