{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cced82-df67-4db3-be0c-95c03a359e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/explosion/spacy-models/releases//download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed8c785-59fe-4a2c-abbc-a15fd5fa97cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2b0894b6fc8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #en_core_web_lg\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc8e402-e78e-4f7e-8934-3f6f39c6cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textacy.subject_verb_object_extract import findSVOs\n",
    "from subject_verb_object_extract import findSVOs\n",
    "from src_rel_tgt_extract import findSVAOs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "punct='!\"\"#$%&\\'()*+-/:;<=>?@[\\\\]^_”’“`{|}~``''—'  \n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "l_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "import py_stringmatching as sm\n",
    " \n",
    "SW=sm.SmithWaterman()\n",
    "GJ=sm.GeneralizedJaccard()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()\n",
    "ocf=sm.OverlapCoefficient()\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_ratio import PartialRatio\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_token_sort import PartialTokenSort\n",
    "pr= PartialRatio()\n",
    "pts= PartialTokenSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6513edb7-b32b-4c81-a3a1-6a934223e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8af72d-c491-4904-ad7c-590f5d6913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_ent_list = [ 'CARDINAL', 'DATE','WORK_OF_ART','MONEY','ORDINAL','FAC','TIME','LOC','QUANTITY','LAW','PERCENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c675af39-ba2e-46ac-b46f-4c40e62c943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_title(data):\n",
    "    mod_list = []\n",
    "    for x in data.split('\\n\\n'):\n",
    "        word_count = len(x.split())\n",
    "        if word_count > 15:\n",
    "            # print(x, '->', word_count )\n",
    "            mod_list.append(x)\n",
    "    final_text = ''.join(mod_list)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6ffe6-d260-424f-9c89-228a659efcc9",
   "metadata": {},
   "source": [
    "### Wiki Pedia Data for chip shortage & semiconductors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1bee42b-a008-4025-8624-f1809bac3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(name_topic, verbose=True):\n",
    "    def link_to_wikipedia(link):\n",
    "        try:\n",
    "            page = api_wikipedia.page(link)\n",
    "            if page.exists():\n",
    "                return {'page': link, 'text': page.text, 'link': page.fullurl, 'categories': list(page.categories.keys())}\n",
    "        except:\n",
    "            return None\n",
    "      \n",
    "    api_wikipedia = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    name_of_page = api_wikipedia.page(name_topic)\n",
    "    if not name_of_page.exists():\n",
    "        print('Page {} is not present'.format(name_of_page))\n",
    "        return\n",
    "  \n",
    "    links_to_page = list(name_of_page.links.keys())\n",
    "    procceed = tqdm(desc='Scraped links', unit='', total=len(links_to_page)) if verbose else None\n",
    "    origin = [{'page': name_topic, 'text': name_of_page.text, 'link': name_of_page.fullurl, 'categories': list(name_of_page.categories.keys())}]\n",
    "  \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        links_future = {executor.submit(link_to_wikipedia, link): link for link in links_to_page}\n",
    "        for future in concurrent.futures.as_completed(links_future):\n",
    "            info = future.result()\n",
    "            origin.append(info) if info else None\n",
    "            procceed.update(1) if verbose else None\n",
    "    procceed.close() if verbose else None\n",
    "  \n",
    "    namespaces = ('Wikipedia', 'Special', 'Talk', 'LyricWiki', 'File', 'MediaWiki',\n",
    "                 'Template', 'Help', 'User', 'Category talk', 'Portal talk')\n",
    "    origin = pd.DataFrame(origin)\n",
    "    origin = origin[(len(origin['text']) > 20)\n",
    "                      & ~(origin['page'].str.startswith(namespaces, na=True))]\n",
    "    origin['categories'] = origin.categories.apply(lambda a: [b[9:] for b in a])\n",
    "\n",
    "    origin['topic'] = name_topic\n",
    "    print('Scraped pages', len(origin))\n",
    "  \n",
    "    return origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d875f76-a157-4513-bb65-3791941690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_chip_shortage = scrape_wikipedia('chip shortage')\n",
    "# data_chip_act = scrape_wikipedia('CHIPS and Science Act')\n",
    "# data_semi_supply = scrape_wikipedia('2021–2022 global supply chain crisis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12fd06dc-b1fd-44af-8cd5-a6bbe1983c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_chip_shortage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7895ced2-487c-4479-8305-f7f306dc43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_chip_act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49ed3868-6c52-4863-9d5f-9a9a40eb0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_semi_supply.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e84ed478-7b1b-4606-a384-0addf4ef83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation to combine key words.\n",
    "# df_combined_wiki = pd.concat([data_chip_shortage,data_chip_act,data_semi_supply])\n",
    "# df_combined_wiki.to_csv('scraped_data_cs_all.csv',index=False)\n",
    "# len(df_combined_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3973de13-46cd-4646-b80b-29029c6b4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "# import en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "from tqdm import tqdm\n",
    "# import neptune.new as neptune\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990c4f2c-5192-4f0d-be04-69c27b6fe170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>news</th>\n",
       "      <th>url</th>\n",
       "      <th>categories</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>COVID-19 pandemic in Belarus</td>\n",
       "      <td>The COVID-19 pandemic in Belarus is part of th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/COVID-19_pandemi...</td>\n",
       "      <td>['2020 in Belarus', '2021 in Belarus', 'Articl...</td>\n",
       "      <td>2021–2022 global supply chain crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>COVID-19 pandemic in Belgium</td>\n",
       "      <td>The COVID-19 pandemic in Belgium has resulted ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/COVID-19_pandemi...</td>\n",
       "      <td>['2020 in Belgium', '2021 in Belgium', 'All Wi...</td>\n",
       "      <td>2021–2022 global supply chain crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>COVID-19 pandemic in Bihar</td>\n",
       "      <td>The first COVID-19 case in the Indian state of...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/COVID-19_pandemi...</td>\n",
       "      <td>['All Wikipedia articles written in Indian Eng...</td>\n",
       "      <td>2021–2022 global supply chain crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>COVID-19 pandemic in Bhutan</td>\n",
       "      <td>The COVID-19 pandemic in Bhutan is part of the...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/COVID-19_pandemi...</td>\n",
       "      <td>['2020 in Bhutan', '2021 in Bhutan', 'All arti...</td>\n",
       "      <td>2021–2022 global supply chain crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>COVID-19 pandemic in Bonaire</td>\n",
       "      <td>The COVID-19 pandemic in Bonaire is part of th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/COVID-19_pandemi...</td>\n",
       "      <td>['2020 in Bonaire', '2021 in Bonaire', 'Articl...</td>\n",
       "      <td>2021–2022 global supply chain crisis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             page  \\\n",
       "373  COVID-19 pandemic in Belarus   \n",
       "374  COVID-19 pandemic in Belgium   \n",
       "375    COVID-19 pandemic in Bihar   \n",
       "376   COVID-19 pandemic in Bhutan   \n",
       "377  COVID-19 pandemic in Bonaire   \n",
       "\n",
       "                                                  news  \\\n",
       "373  The COVID-19 pandemic in Belarus is part of th...   \n",
       "374  The COVID-19 pandemic in Belgium has resulted ...   \n",
       "375  The first COVID-19 case in the Indian state of...   \n",
       "376  The COVID-19 pandemic in Bhutan is part of the...   \n",
       "377  The COVID-19 pandemic in Bonaire is part of th...   \n",
       "\n",
       "                                                   url  \\\n",
       "373  https://en.wikipedia.org/wiki/COVID-19_pandemi...   \n",
       "374  https://en.wikipedia.org/wiki/COVID-19_pandemi...   \n",
       "375  https://en.wikipedia.org/wiki/COVID-19_pandemi...   \n",
       "376  https://en.wikipedia.org/wiki/COVID-19_pandemi...   \n",
       "377  https://en.wikipedia.org/wiki/COVID-19_pandemi...   \n",
       "\n",
       "                                            categories  \\\n",
       "373  ['2020 in Belarus', '2021 in Belarus', 'Articl...   \n",
       "374  ['2020 in Belgium', '2021 in Belgium', 'All Wi...   \n",
       "375  ['All Wikipedia articles written in Indian Eng...   \n",
       "376  ['2020 in Bhutan', '2021 in Bhutan', 'All arti...   \n",
       "377  ['2020 in Bonaire', '2021 in Bonaire', 'Articl...   \n",
       "\n",
       "                                    topic  \n",
       "373  2021–2022 global supply chain crisis  \n",
       "374  2021–2022 global supply chain crisis  \n",
       "375  2021–2022 global supply chain crisis  \n",
       "376  2021–2022 global supply chain crisis  \n",
       "377  2021–2022 global supply chain crisis  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data = pd.read_csv('scraped_data_cs_all.csv')\n",
    "wiki_data.columns = ['page', 'news', 'url', 'categories', 'topic']\n",
    "wiki_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c0f80e7-2219-496a-98c5-5238475df428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikidata_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51426eb4-081c-45e1-a89d-35b8f15041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_data = wiki_data[~wiki_data['news'].str.contains('COVID-19')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f518c48-cbfa-4421-ad7a-a9e90d3b4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki_data(x):\n",
    "    list_data = x.split('\\n\\n')\n",
    "    final_list = []\n",
    "    for sent in list_data:\n",
    "        text = sent.split('\\n')\n",
    "        for tx in text:\n",
    "            # print(tx)\n",
    "            if len(tx.split()) > 5:\n",
    "                # f_text = ''.join(tx)\n",
    "                # print(f_text)\n",
    "                final_list.append(tx)\n",
    "                # break\n",
    "    # final_text = ' '.join(final_list)\n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48688070-11e6-458c-a793-98161ab0df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data['news'] = wiki_data['news'].apply(lambda x : clean_wiki_data(x))\n",
    "wiki_data = wiki_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02763aa9-8b96-4322-8458-618e7726d014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A chip shortage, also referred to as semiconductor shortage or chip famine, is a phenomenon in the integrated circuit (chip) industry when demand for silicon chips outstrips supply.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data['news'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed4cf27-84e4-49c0-b067-951644e3351a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>news</th>\n",
       "      <th>url</th>\n",
       "      <th>categories</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chip shortage</td>\n",
       "      <td>[A chip shortage, also referred to as semicond...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Chip_shortage</td>\n",
       "      <td>['All Wikipedia articles in need of updating',...</td>\n",
       "      <td>chip shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Broadcom Inc.</td>\n",
       "      <td>[Broadcom Inc. is an American designer, develo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Broadcom_Inc.</td>\n",
       "      <td>['1961 establishments in California', '2009 in...</td>\n",
       "      <td>chip shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011 Tōhoku earthquake and tsunami</td>\n",
       "      <td>[The 2011 Tōhoku earthquake and tsunami (Japan...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2011_T%C5%8Dhoku...</td>\n",
       "      <td>['2011 Tōhoku earthquake and tsunami', '2011 d...</td>\n",
       "      <td>chip shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020–present global chip shortage</td>\n",
       "      <td>[The 2020–present global chip shortage is an o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2020%E2%80%93pre...</td>\n",
       "      <td>['2020 in computing', '2020s economic history'...</td>\n",
       "      <td>chip shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital expenditure</td>\n",
       "      <td>[Capital expenditure or capital expense (capex...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Capital_expenditure</td>\n",
       "      <td>['Accounting terminology', 'All articles needi...</td>\n",
       "      <td>chip shortage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page  \\\n",
       "0                       chip shortage   \n",
       "1                       Broadcom Inc.   \n",
       "2  2011 Tōhoku earthquake and tsunami   \n",
       "3   2020–present global chip shortage   \n",
       "4                 Capital expenditure   \n",
       "\n",
       "                                                news  \\\n",
       "0  [A chip shortage, also referred to as semicond...   \n",
       "1  [Broadcom Inc. is an American designer, develo...   \n",
       "2  [The 2011 Tōhoku earthquake and tsunami (Japan...   \n",
       "3  [The 2020–present global chip shortage is an o...   \n",
       "4  [Capital expenditure or capital expense (capex...   \n",
       "\n",
       "                                                 url  \\\n",
       "0        https://en.wikipedia.org/wiki/Chip_shortage   \n",
       "1        https://en.wikipedia.org/wiki/Broadcom_Inc.   \n",
       "2  https://en.wikipedia.org/wiki/2011_T%C5%8Dhoku...   \n",
       "3  https://en.wikipedia.org/wiki/2020%E2%80%93pre...   \n",
       "4  https://en.wikipedia.org/wiki/Capital_expenditure   \n",
       "\n",
       "                                          categories          topic  \n",
       "0  ['All Wikipedia articles in need of updating',...  chip shortage  \n",
       "1  ['1961 establishments in California', '2009 in...  chip shortage  \n",
       "2  ['2011 Tōhoku earthquake and tsunami', '2011 d...  chip shortage  \n",
       "3  ['2020 in computing', '2020s economic history'...  chip shortage  \n",
       "4  ['Accounting terminology', 'All articles needi...  chip shortage  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e25402e-f68f-4bea-aa3c-a12f7dd585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svo_ent_tkn_wiki(wiki_data):\n",
    "    wk_doc_sub_v_sub=list() \n",
    "    wk_root_verbs=list()\n",
    "    wk_token_entity=list()\n",
    "    for j in range((wiki_data.shape[0])):\n",
    "        data = wiki_data['news'][j]\n",
    "        for sent in data:\n",
    "            # sample_data_new = rem_title(sent)\n",
    "            int_text=[x.translate(str.maketrans('','',punct)) for x in  sent.split()]\n",
    "            # print(' '.join(int_text))\n",
    "            doc=nlp(' '.join((int_text)))\n",
    "            for tok in doc:\n",
    "                if tok.dep_=='ROOT' and tok.pos_=='VERB':\n",
    "                    wk_root_verbs.append(str(tok))\n",
    "            # print(wk_root_verbs)\n",
    "\n",
    "            # To solve coref issue\n",
    "            doc_coref_resolved=nlp(doc._.coref_resolved)\n",
    "\n",
    "            #Identifying entity types. Need some new code to add new entities\n",
    "            for tok in doc_coref_resolved.ents:\n",
    "                if tok.label_ not in reject_ent_list:\n",
    "                    wk_token_entity.append([str(tok),tok.label_])\n",
    "            # print(wk_token_entity)\n",
    "\n",
    "            wk_SVO=[]\n",
    "            #SVO using a custom code to handle active and passive voice\n",
    "            for sent in doc_coref_resolved.sents:\n",
    "                text = findSVOs(sent)\n",
    "                wk_SVO.append(text)\n",
    "            wk_doc_sub_v_sub.append([item for sublist in wk_SVO for item in sublist if item != []])\n",
    "            # print(wk_doc_sub_v_sub)\n",
    "\n",
    "                ## The below snippets appends needed columns from the news data with the subject object and verb tripples    \n",
    "            wk_l_sub_v_sub=list()\n",
    "            for i in range(len(wk_doc_sub_v_sub)):\n",
    "                # new=list(sample_data.loc[i].values)[:4]\n",
    "\n",
    "                for j in range(len(wk_doc_sub_v_sub[i])):\n",
    "                    #df_needed_fields.append([new+[t for t in  doc_sub_v_sub[i][j]]])\n",
    "                    wk_l_sub_v_sub.append([str(sub_obj) for sub_obj in  wk_doc_sub_v_sub[i][j]])\n",
    "        # print(wk_l_sub_v_sub)\n",
    "        # print(wk_token_entity)\n",
    "    return wk_l_sub_v_sub, wk_token_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e075f9e5-f417-4a32-a658-e25e859fb474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_sub_vb_obj, wiki_token_entity = create_svo_ent_tkn_wiki(wiki_data)\n",
    "df_doc_svb_wiki = pd.DataFrame(wiki_sub_vb_obj,columns=['subject','verb','object'])\n",
    "df_tkn_ent_wiki = pd.DataFrame(wiki_token_entity,columns=['Token','Entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b8c5e1-5b3f-431c-9dc4-6c88baa07477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NORP', 'PERSON', 'ORG', 'GPE', 'PRODUCT', 'LANGUAGE', 'EVENT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tkn_ent_wiki.Entity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4fa62c-545a-4f35-bb2d-051892204a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doc_svb_wiki.to_csv('Data/sbvbob_wiki_lib2.csv',index=False)\n",
    "# df_tkn_ent_wiki.to_csv('Data/tkn_ent_wiki_lib2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47bde37-4d83-4067-9904-73b4354de926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_svb_wiki = pd.read_csv('Data/sbvbob_wiki_lib2.csv')\n",
    "df_tkn_ent_wiki = pd.read_csv('Data/tkn_ent_wiki_lib2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58188786-001b-41c8-b770-53f8d911847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73587"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_doc_svb_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e1163-b9da-4ab7-9463-42816bc28206",
   "metadata": {},
   "source": [
    "##### Wiki News Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86844152-011d-428c-8b28-fa32329d51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below dataframe lowers the tokens in both the dataframe\n",
    "df_tkn_ent_wiki['Token']=df_tkn_ent_wiki['Token'].str.lower()\n",
    "df_tkn_ent_wiki=df_tkn_ent_wiki.drop_duplicates()\n",
    "df_tkn_ent_wiki=df_tkn_ent_wiki.drop_duplicates(subset='Token', keep=\"first\")\n",
    "df_doc_svb_wiki['subject']=df_doc_svb_wiki['subject'].str.lower()\n",
    "df_doc_svb_wiki['object']=df_doc_svb_wiki['object'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de5b8dd6-65a0-44eb-b305-c9cd8ed4185f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73587"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_doc_svb_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43542e39-44ad-4337-b679-a5d46df348d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>japanese</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gateway</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>intel</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amd</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token  Entity\n",
       "0  american    NORP\n",
       "1  japanese    NORP\n",
       "4   gateway  PERSON\n",
       "5     intel     ORG\n",
       "6       amd     ORG"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tkn_ent_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8abf1a-29e9-493b-adff-da8c97ce1e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 73587\n",
      "2. 73587\n",
      "3. 1845\n"
     ]
    }
   ],
   "source": [
    "#Creating the sub verb and object dataframe and joining the entity to find the subject and object entities\n",
    "df_doc_svb_wiki=df_doc_svb_wiki.merge(df_tkn_ent_wiki,how='left', left_on='subject',right_on='Token')\n",
    "df_doc_svb_wiki=df_doc_svb_wiki.merge(df_tkn_ent_wiki,how='left', left_on='object',right_on='Token')\n",
    "print('1.',len(df_doc_svb_wiki))\n",
    "#renaming and dropping unnecessary columns\n",
    "df_doc_svb_wiki=df_doc_svb_wiki.rename(columns={\"Entity_x\":\"Subject_Entity\",\"Entity_y\":\"Object_Entity\"})\n",
    "df_doc_svb_wiki=df_doc_svb_wiki.drop(columns=['Token_x','Token_y'])\n",
    "print('2.',len(df_doc_svb_wiki))\n",
    "# Removing rows that does not have an entity defined and where subject and object are the same\n",
    "df_doc_svb_wiki=df_doc_svb_wiki[df_doc_svb_wiki['Subject_Entity'].notnull() & df_doc_svb_wiki['Object_Entity'].notnull()]\n",
    "df_doc_svb_wiki=df_doc_svb_wiki[df_doc_svb_wiki['subject'].notnull() & df_doc_svb_wiki['object'].notnull()]\n",
    "df_doc_svb_wiki=df_doc_svb_wiki[df_doc_svb_wiki['subject']!=df_doc_svb_wiki['object']]\n",
    "print('3.',len(df_doc_svb_wiki))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c3cf5-709f-4a3f-a960-b5f3e16b416a",
   "metadata": {},
   "source": [
    "##### Stop word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0843a0c-0f50-4df2-a4ec-2bfb12c96e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. 1644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spacy_stop=list(nlp.Defaults.stop_words)\n",
    "complete_stop_word=list(set(l_stopwords+spacy_stop))\n",
    "\n",
    "df_doc_svb_wiki=df_doc_svb_wiki[~df_doc_svb_wiki['verb'].isin(complete_stop_word)]\n",
    "\n",
    "data_verbs=df_doc_svb_wiki['verb'].to_list()\n",
    "lemm_verbs=[]\n",
    "j=0\n",
    "for i in (data_verbs):\n",
    "    doc_verb=nlp(i)\n",
    "    verbs=[]\n",
    "    for tok in doc_verb:\n",
    "        verbs.append(tok.lemma_)\n",
    "    verbs=' '.join(verbs)\n",
    "    lemm_verbs.append(verbs)\n",
    "    j=j+1\n",
    "    if(j%10000==0):\n",
    "        print(j)\n",
    "df_doc_svb_wiki['lemm_verbs']=lemm_verbs\n",
    "df_doc_svb_wiki=df_doc_svb_wiki[~df_doc_svb_wiki['lemm_verbs'].isin(complete_stop_word)]\n",
    "print('4.',len(df_doc_svb_wiki))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3facb17-5678-48f9-a165-a94ca94f80c4",
   "metadata": {},
   "source": [
    "#### Find word Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee9d02a2-de59-47ab-8120-61440c711aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import spacy as spacy_nlp\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "# spacy_nlp = spacy.load('en')\n",
    "nlp_lemma_word_ann = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "nlp_lemma_word_ann.add_pipe(WordnetAnnotator(nlp_lemma_word_ann, name=\"spacy_wordnet\"), after='tagger')\n",
    "\n",
    "nlp_lemma_NO_word_annot = spacy.load('en_core_web_sm', disable=['parser', 'ner','WordnetAnnotator'])\n",
    "\n",
    "lemm_verbs_no_stopwords=df_doc_svb_wiki['lemm_verbs'].to_list()\n",
    "#lemm_verbs_no_stopwords=[i.replace('! ', '') for i in lemm_verbs_no_stopwords]\n",
    "sorted_lemma_count=sorted(Counter(lemm_verbs_no_stopwords).items(), key=operator.itemgetter(1),reverse=True) \n",
    " \n",
    "#Get just the verbs from the previous list and ignore the verb count\n",
    "lemm_verbs_synonym=[a_tuple[0] for a_tuple in sorted_lemma_count]\n",
    "all_synonyms = []\n",
    "prev_all_synonyms=[]\n",
    "synn_list=[]\n",
    "#Find the synonyms of the top verbs and replace that verb if any synonym is found\n",
    "for i in range(len(sorted_lemma_count)):\n",
    "     \n",
    "    #all_synonyms=[item for sublist in all_synonyms for item in sublist]\n",
    "     \n",
    "    if sorted_lemma_count[i][0] not in prev_all_synonyms:#Check is needed so that the same word does not get updated\n",
    "        #print(sorted_lemma_count[i][0])\n",
    "        if (len(sorted_lemma_count[i][0].split())==1):\n",
    "            for word in  nlp_lemma_word_ann(sorted_lemma_count[i][0]):\n",
    "                synonyms = []\n",
    "                for sysnet in word._.wordnet.synsets():\n",
    "\n",
    "                    for l in sysnet.lemmas():\n",
    "                        synonyms.append(l.name())\n",
    "                        all_synonyms.append(l.name())\n",
    "            synonyms=list(set(synonyms))\n",
    "            #print(synonyms)\n",
    "            all_synonyms=list(set(all_synonyms))\n",
    "            synn_list.append((sorted_lemma_count[i][0],synonyms))\n",
    "\n",
    "            for n,j in enumerate(lemm_verbs_no_stopwords):\n",
    "                #print(j,prev_all_synonyms)\n",
    "                if j in synonyms and j not in prev_all_synonyms : #and lemm_verbs_no_stopwords[n]!=sorted_lemma_count[i][0]\n",
    "                    #print(sorted_lemma_count[i][0],j)\n",
    "                    if (len(j.split())==1):\n",
    "                    #print(j,sorted_lemma_count[i][0])\n",
    "\n",
    "                    #idx=lemm_verbs_no_stopwords.index(j)\n",
    "                    #print(j ,sorted_lemma_count[i][0])\n",
    "                        #print(j,'before',lemm_verbs_no_stopwords[n],'after',sorted_lemma_count[i][0])\n",
    "                        lemm_verbs_no_stopwords[n]=sorted_lemma_count[i][0]\n",
    "                        #print('after',lemm_verbs_no_stopwords[n])\n",
    "              \n",
    "            prev_all_synonyms=all_synonyms\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "df_doc_svb_wiki['synn_verbs']=lemm_verbs_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4243254a-3f84-4386-bf0e-766d2822dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(full_s_o_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2d60-ba31-4cd2-8340-e4d5d9440850",
   "metadata": {},
   "source": [
    "##### Simialiarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d3cd55f-4209-4467-8120-51be6efa6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "Wall time: 9.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating subject and object list\n",
    "subject_list=df_doc_svb_wiki['subject'].to_list()\n",
    "subject_list=[item.lower() for item in subject_list]\n",
    "\n",
    "\n",
    "object_list=df_doc_svb_wiki['object'].to_list()\n",
    "object_list=[item.lower() for item in object_list]\n",
    " \n",
    "full_s_o_list=subject_list+object_list\n",
    "\n",
    "\n",
    "import py_stringmatching as sm\n",
    " \n",
    "SW=sm.SmithWaterman()\n",
    "GJ=sm.GeneralizedJaccard()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()\n",
    "ocf=sm.OverlapCoefficient()\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_ratio import PartialRatio\n",
    " \n",
    "from py_stringmatching.similarity_measure.partial_token_sort import PartialTokenSort\n",
    "pr= PartialRatio()\n",
    "pts= PartialTokenSort()\n",
    "\n",
    "\n",
    "full_s_o_list_count=sorted(Counter(full_s_o_list).items(), key=operator.itemgetter(1),reverse=True) \n",
    "\n",
    "#Get just the subject and object from the previous list and ignore the  count\n",
    "full_s_o_list_final=[a_tuple[0] for a_tuple in full_s_o_list_count] \n",
    "\n",
    "\n",
    "ws_tok_set= sm.WhitespaceTokenizer(return_set=True)\n",
    "calc_score_temp=[]\n",
    "all_sym=[]\n",
    "sim_lis=[]\n",
    "prev_all_sym=[]\n",
    "  \n",
    "for i in range(len(full_s_o_list_final)):\n",
    "     \n",
    "    if full_s_o_list_final[i]  not in all_sym: #Check is needed so that the same word does not get updated\n",
    "        similarity_list = []\n",
    "        calc_score_temp=[]\n",
    "        for j in range(len(full_s_o_list_final)):\n",
    "             \n",
    "            # score=(2*(SW.get_raw_score( full_s_o_list_final[i],full_s_o_list_final[j]))/(len(full_s_o_list_final[i])+len(full_s_o_list_final[j])))\n",
    "            score=jac.get_raw_score(ws_tok_set.tokenize(full_s_o_list_final[i]), ws_tok_set.tokenize(full_s_o_list_final[j]))\n",
    "            \n",
    "            calc_score_temp.append((full_s_o_list_final[i],full_s_o_list_final[j],score))\n",
    "            \n",
    "            if score > .5:\n",
    "                all_sym.append(full_s_o_list_final[j])\n",
    "                similarity_list.append(full_s_o_list_final[j])\n",
    "        sim_lis.append((full_s_o_list_final[i],similarity_list))\n",
    "        for n,k in enumerate(subject_list):\n",
    "            if k in similarity_list and k not in prev_all_sym :\n",
    "                 subject_list[n]=full_s_o_list_final[i]\n",
    "        for n,k in enumerate(object_list):\n",
    "            if k in similarity_list and k not in prev_all_sym :\n",
    "                object_list[n]=full_s_o_list_final[i]\n",
    "    if(i%1000==0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea706ed7-e057-4fd0-834e-c79e65459bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_svb_wiki['subject_sim']=subject_list\n",
    "df_doc_svb_wiki['object_sim']=object_list\n",
    "\n",
    "df_doc_sub_v_sub_wiki_final = df_doc_svb_wiki[df_doc_svb_wiki['subject_sim']!=df_doc_svb_wiki['object_sim']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b9bc531-0793-4e83-b42a-3b5ec01222f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdivedi\\Anaconda3\\envs\\nlp_proj\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_doc_sub_v_sub_wiki_final['verb_source_type']='Wiki'\n",
    "\n",
    "df_doc_sub_v_sub_wikigraph_out=df_doc_sub_v_sub_wiki_final[['subject_sim','Subject_Entity','object_sim','Object_Entity','synn_verbs','verb_source_type']]\n",
    "df_doc_sub_v_sub_wikigraph_out=df_doc_sub_v_sub_wikigraph_out.rename(columns={'subject_sim':'subject','Subject_Entity':'subject_entity','object_sim':'object','Object_Entity':'object_entity','synn_verbs':'synn_verbs'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e157a658-4f85-4a46-af71-92d5b3751ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_sub_v_sub_wikigraph_out = df_doc_sub_v_sub_wikigraph_out[(df_doc_sub_v_sub_wikigraph_out['object'].str.len()>2) & (df_doc_sub_v_sub_wikigraph_out['subject'].str.len()>2)] \n",
    "df_doc_sub_v_sub_wikigraph_out = df_doc_sub_v_sub_wikigraph_out[(df_doc_sub_v_sub_wikigraph_out['object'].str.len()>2) & (df_doc_sub_v_sub_wikigraph_out['subject'].str.len()>2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84a729db-85a7-4c80-9981-09b7596df5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_wiki = df_doc_sub_v_sub_wikigraph_out[['subject','object','synn_verbs','subject_entity','object_entity','verb_source_type']]\n",
    "df_graph_wiki.columns = [':START_ID',':END_ID',':TYPE','subject_entity','object_entity','source_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "421402a8-8c6f-42d8-8ecb-08ab0c12c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>:START_ID</th>\n",
       "      <th>:END_ID</th>\n",
       "      <th>:TYPE</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>source_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chips</td>\n",
       "      <td>resin</td>\n",
       "      <td>use</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avago technologies</td>\n",
       "      <td>infineon technologies</td>\n",
       "      <td>acquire</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avago technologies</td>\n",
       "      <td>broadcom corporation</td>\n",
       "      <td>buy</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qualcomm</td>\n",
       "      <td>broadcom</td>\n",
       "      <td>purchase</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the european union</td>\n",
       "      <td>broadcom</td>\n",
       "      <td>stop</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Wiki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            :START_ID                :END_ID     :TYPE subject_entity  \\\n",
       "0               chips                  resin       use            ORG   \n",
       "1  avago technologies  infineon technologies   acquire            ORG   \n",
       "2  avago technologies   broadcom corporation       buy            ORG   \n",
       "3            qualcomm               broadcom  purchase         PERSON   \n",
       "4  the european union               broadcom      stop            ORG   \n",
       "\n",
       "  object_entity source_type  \n",
       "0           ORG        Wiki  \n",
       "1           ORG        Wiki  \n",
       "2           ORG        Wiki  \n",
       "3           ORG        Wiki  \n",
       "4           ORG        Wiki  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_graph_wiki=df_graph_wiki[df_graph_wiki[':END_ID']!=df_graph_wiki[':TYPE']] \n",
    "df_graph_wiki = df_graph_wiki.drop_duplicates()\n",
    "df_graph_wiki.reset_index(drop=True, inplace=True)\n",
    "df_graph_wiki.to_csv('Data/wiki_total_1208.csv',index=False)\n",
    "df_graph_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22d41f84-2808-4a8a-9164-cb09d46f665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn1 = df_graph_news[[':START_ID','subject_entity']]\n",
    "# df_tkn1.columns = ['name',':LABEL']\n",
    "# df_tkn2 = df_graph_news[[':END_ID','object_entity']]\n",
    "# df_tkn2.columns = ['name',':LABEL']\n",
    "# df_tkn3 = df_graph_wiki[[':START_ID','subject_entity']]\n",
    "# df_tkn3.columns = ['name',':LABEL']\n",
    "# df_tkn4 = df_graph_wiki[[':END_ID','object_entity']]\n",
    "# df_tkn4.columns = ['name',':LABEL']\n",
    "# df_tkn_merged = pd.concat([df_tkn1,df_tkn2,df_tkn3,df_tkn4], ignore_index=True)\n",
    "# df_tkn_merged = df_tkn_merged.drop_duplicates(subset=['name'])\n",
    "# df_tkn_merged.reset_index(inplace = True)\n",
    "# df_tkn_merged.reset_index(inplace = True)\n",
    "# df_tkn_merged = df_tkn_merged.rename(columns={'level_0':'Id:ID'})\n",
    "# df_tkn_merged['Id:ID'] = df_tkn_merged['name'].apply(lambda x : x.upper())\n",
    "# df_tkn_merged = df_tkn_merged.drop('index',axis=1)\n",
    "# df_tkn_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d822bfb-2bee-410a-9028-37ddf3cd29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id(x):\n",
    "    text = x.split()\n",
    "    text = '_'.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b39faee-c569-4b07-88c8-adaa884c4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn_merged['Id:ID'] = df_tkn_merged['Id:ID'].apply(lambda x : create_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0000cac-7777-4bcf-9ca4-0970c5a07613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e84f8ce9-6a2a-4e65-8e28-4bb24ecf3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn_merged = df_tkn_merged.drop_duplicates(subset=['Id:ID'])\n",
    "# df_tkn_merged = df_tkn_merged[(df_tkn_merged['Id:ID'].str.len()>2)] \n",
    "# df_tkn_merged[df_tkn_merged['Id:ID'] == 'SI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0ecb6710-f3db-4cb5-9d85-975456670b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tkn_merged.to_csv('node_all_1207_2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb308fce-23fa-44f5-a809-ffd9fade84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_tkn_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a506e7c-e6df-4848-a8e6-0e771870d09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ee087-a0c2-4f6a-83ef-ad3eda2ec6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "nlp_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
